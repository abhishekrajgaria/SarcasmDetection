{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_prj_attn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ilfg7i6tWPEJ","executionInfo":{"status":"ok","timestamp":1606717214185,"user_tz":-330,"elapsed":28795,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"93eecb1e-4027-4427-8ad9-16cb53535de7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWHqPSGrgK4D","executionInfo":{"status":"ok","timestamp":1606717218469,"user_tz":-330,"elapsed":2581,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"d5b1c62b-0217-4b89-cdbf-2ba218cbafd9"},"source":["cd /content/drive/My Drive/NLP_project/atten"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/NLP_project/atten\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPsakRLuhLdS","executionInfo":{"status":"ok","timestamp":1606717222974,"user_tz":-330,"elapsed":3759,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"5be3f8f9-e60c-42a3-809d-046296b95b20"},"source":["ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["glove.840B.300d.txt  sarcasm_data.json  \u001b[0m\u001b[01;34msrc\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D15InJAehObz","executionInfo":{"status":"ok","timestamp":1606717225401,"user_tz":-330,"elapsed":4414,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["import json\n","import re\n","import csv\n","import nltk\n","import math\n","# import gensim = 3.8.3\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.util import ngrams\n","# from num2words import num2words\n","from gensim.models import Word2Vec\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.tokenize import sent_tokenize, word_tokenize"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xy6pxtWV52He","executionInfo":{"status":"ok","timestamp":1606717225407,"user_tz":-330,"elapsed":1208,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgSzF6rlsZ6t","executionInfo":{"status":"ok","timestamp":1606717229309,"user_tz":-330,"elapsed":3645,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"9753c85e-63b3-465d-cce3-6a4967970c0e"},"source":["nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"PmFSVDTchOef","executionInfo":{"status":"ok","timestamp":1606717230812,"user_tz":-330,"elapsed":3864,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"dd494cf4-9aaf-4879-8df4-f2bfb0581f89"},"source":["df = pd.read_json ('sarcasm_data.json')\n","df = df.T\n","def get_labels(text):\n","    if(text == True):\n","        return 1\n","    return 0\n","\n","df['sarcasm'] = df['sarcasm'].apply(get_labels)\n","df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>utterance</th>\n","      <th>speaker</th>\n","      <th>context</th>\n","      <th>context_speakers</th>\n","      <th>show</th>\n","      <th>sarcasm</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>160</th>\n","      <td>It's just a privilege to watch your mind at work.</td>\n","      <td>SHELDON</td>\n","      <td>[I never would have identified the fingerprint...</td>\n","      <td>[LEONARD, SHELDON]</td>\n","      <td>BBT</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>170</th>\n","      <td>I don't think I'll be able to stop thinking ab...</td>\n","      <td>PENNY</td>\n","      <td>[This is one of my favorite places to kick bac...</td>\n","      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n","      <td>BBT</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>180</th>\n","      <td>Since it's not bee season, you can have my epi...</td>\n","      <td>SHELDON</td>\n","      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n","      <td>[LEONARD, HOWARD, LEONARD]</td>\n","      <td>BBT</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>190</th>\n","      <td>Lois Lane is falling, accelerating at an initi...</td>\n","      <td>SHELDON</td>\n","      <td>[A marathon? How many Superman movies are ther...</td>\n","      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n","      <td>BBT</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1105</th>\n","      <td>I'm just inferring this is a couch because the...</td>\n","      <td>SHELDON</td>\n","      <td>[Great Caesar's ghost, look at this place., So...</td>\n","      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n","      <td>BBT</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              utterance  speaker  ... show sarcasm\n","160   It's just a privilege to watch your mind at work.  SHELDON  ...  BBT       1\n","170   I don't think I'll be able to stop thinking ab...    PENNY  ...  BBT       1\n","180   Since it's not bee season, you can have my epi...  SHELDON  ...  BBT       0\n","190   Lois Lane is falling, accelerating at an initi...  SHELDON  ...  BBT       0\n","1105  I'm just inferring this is a couch because the...  SHELDON  ...  BBT       1\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"VJkhw-VAi_Yb","executionInfo":{"status":"ok","timestamp":1606717230814,"user_tz":-330,"elapsed":3287,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":[""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuC00pV6mIug","executionInfo":{"status":"ok","timestamp":1606717230815,"user_tz":-330,"elapsed":2624,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["column_dict = {'utterance':0,'speaker':1,'context':2,'context_speaker':3,'show':4,'sarcasm':5}"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLfazoK8lpm3","executionInfo":{"status":"ok","timestamp":1606717230816,"user_tz":-330,"elapsed":2032,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"0f28599f-c521-486a-e9dd-3d3d487c3098"},"source":["data = df.to_numpy()\n","print(data[0][2][0])\n","print(data.shape)\n","print(len(data[:,[0]]))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["I never would have identified the fingerprints of string theory in the aftermath of the Big Bang.\n","(690, 6)\n","690\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58L_szlmq-WX","executionInfo":{"status":"ok","timestamp":1606717231605,"user_tz":-330,"elapsed":2178,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"26a0852f-0dbb-401e-b855-c0a57cefb8be"},"source":["#creating total vocab for the data\n","vocab = set()\n","\n","for samp in data[:,0]:\n","  # print(samp)\n","  # break\n","  sent_token = sent_tokenize(samp)\n","  for j in sent_token:\n","    tokens = word_tokenize(j)\n","    for token in tokens:\n","      vocab.add(token)\n","\n","print(len(vocab))\n","\n","for samp in data[:,2]:\n","  for sent in samp:\n","    sent_token = sent_tokenize(sent)\n","    for j in sent_token:\n","      tokens = word_tokenize(j)\n","      for token in tokens:\n","        vocab.add(token)\n","print(len(vocab))\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["2194\n","4549\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vB-Jnxcl3yY","executionInfo":{"status":"ok","timestamp":1606717337495,"user_tz":-330,"elapsed":107236,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"6fb050ae-be54-4273-e612-7a78fd7ebdc0"},"source":["glove_dict = {}\n","with open(\"glove.840B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n","  i=0\n","  for line in f:\n","    i+=1\n","    values = line.split()\n","    word = values[0]\n","    if(word in vocab):\n","      st = 1\n","      if(len(values)>301):\n","        st+= len(values) - 301\n","      vector = np.asarray(values[st:],'float32')\n","      glove_dict[word] = vector\n","\n","print(len(glove_dict))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["4428\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CW7uOyreoEzL","executionInfo":{"status":"ok","timestamp":1606717337498,"user_tz":-330,"elapsed":105943,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"a6effc15-5fa7-45bf-d563-73456a8eef7b"},"source":["for word in vocab:\n","  if(word not in glove_dict):\n","    temp_arr = [0.0]*300\n","    glove_dict[word] = np.asarray(temp_arr,'float32')\n","\n","print(len(vocab))\n","print(len(glove_dict))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["4549\n","4549\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfpYMcpTt61q","executionInfo":{"status":"ok","timestamp":1606717341327,"user_tz":-330,"elapsed":108454,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"57296aa4-287c-4b42-d6ea-3cf5078b11a5"},"source":["!pip install texttable\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Collecting texttable\n","  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VOsT4BtuK2z","executionInfo":{"status":"ok","timestamp":1606717380022,"user_tz":-330,"elapsed":146185,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"8f28aa8f-2b0f-4c95-bd83-ac17a3c0c005"},"source":["import torch\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter==latest+{CUDA}     -f https://pytorch-geometric.com/whl/torch-{TORCH}.html\n","!pip install torch-sparse==latest+{CUDA}      -f https://pytorch-geometric.com/whl/torch-{TORCH}.html\n","!pip install torch-cluster==latest+{CUDA}     -f https://pytorch-geometric.com/whl/torch-{TORCH}.html\n","!pip install torch-spline-conv==latest+{CUDA} -f https://pytorch-geometric.com/whl/torch-{TORCH}.html\n","!pip install torch-geometric"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n","Collecting torch-scatter==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (11.9MB)\n","\u001b[K     |████████████████████████████████| 11.9MB 1.6MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.5\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n","Collecting torch-sparse==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (24.3MB)\n","\u001b[K     |████████████████████████████████| 24.3MB 193kB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.8\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n","Collecting torch-cluster==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.5MB)\n","\u001b[K     |████████████████████████████████| 21.5MB 251kB/s \n","\u001b[?25hInstalling collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.8\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n","Collecting torch-spline-conv==latest+cu101\n","\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.4MB)\n","\u001b[K     |████████████████████████████████| 6.4MB 366kB/s \n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.0\n","Collecting torch-geometric\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/55/19d28e5e1ddaa13efd59e56b19f895cf022f74f190a38e77cc68cf8ddf1f/torch_geometric-1.6.2.tar.gz (183kB)\n","\u001b[K     |████████████████████████████████| 184kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n","Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.14)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.4)\n","Collecting rdflib\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n","\u001b[K     |████████████████████████████████| 235kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n","Collecting ase\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.17.0)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n","Collecting isodate\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-1.6.2-cp36-none-any.whl size=318842 sha256=b052a9cc34487f13ce6429229e0cb49a3a043cf1b916cb939cb51709dbfa1df5\n","  Stored in directory: /root/.cache/pip/wheels/ae/09/bb/09bf5950d6404198b0c61ea702536b126b50caf9f6040157d3\n","Successfully built torch-geometric\n","Installing collected packages: isodate, rdflib, ase, torch-geometric\n","Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"72TOtiQmzz04","executionInfo":{"status":"ok","timestamp":1606717380024,"user_tz":-330,"elapsed":142098,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["# import torch\n","\n","class AttentionModule(torch.nn.Module):\n","    \n","    def __init__(self, args):\n","        \n","        super(AttentionModule, self).__init__()\n","        self.args = args\n","        self.filters_3 = 300\n","        self.filters_4 = 64\n","        self.setup_weights()\n","        self.init_parameters()\n","\n","    def setup_weights(self):\n","        \n","        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.filters_3,\n","                                                             self.filters_3))\n","\n","    def init_parameters(self):\n","       \n","        torch.nn.init.xavier_uniform_(self.weight_matrix)\n","\n","    def forward(self, embedding):\n","        \n","        global_context = torch.mean(torch.matmul(embedding, self.weight_matrix), dim=0)\n","        transformed_global = torch.tanh(global_context)\n","        # print(type(transformed_global))\n","        # print(transformed_global.shape)\n","        sigmoid_scores = torch.sigmoid(torch.mm(embedding, transformed_global.view(-1, 1)))\n","        representation = torch.mm(torch.t(embedding), sigmoid_scores)\n","        return representation\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Jr4dmBXo6qM","executionInfo":{"status":"ok","timestamp":1606742053448,"user_tz":-330,"elapsed":2324,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["class Model(torch.nn.Module):\n","    def __init__(self, args):\n","        \n","        super(Model, self).__init__()\n","        self.args = args\n","        self.tensor_neurons = 16\n","        self.bins = 16\n","        self.histogram = False\n","        self.setup_layers()\n","\n","    def setup_layers(self):\n","        \n","        \n","        self.attention = AttentionModule(self.args)\n","      \n","        if(args=='ut'):\n","          self.fully_connected_first = torch.nn.Linear(300,128)\n","        \n","        if(args=='ut-sp'):\n","          self.fully_connected_first = torch.nn.Linear(321,128)\n","\n","        if(args=='ut-ct'):\n","          self.fully_connected_first = torch.nn.Linear(300,128)\n","\n","        if(args=='ut-ct-ext'):\n","          self.fully_connected_first = torch.nn.Linear(350,128)\n","\n","        if(args=='ut-sh'):\n","          self.fully_connected_first = torch.nn.Linear(304,128)\n","\n","        if(args=='ut-sp-ct'):\n","          self.fully_connected_first = torch.nn.Linear(321,128)\n","\n","        if(args=='ut-sp-ct-cp-sh'):\n","          self.fully_connected_first = torch.nn.Linear(349,128)\n","\n","       \n","\n","        self.fully_connected_second = torch.nn.Linear(128,64)\n","        self.fully_connected_third = torch.nn.Linear(64,32)\n","        \n","        self.fully_connected_fourth = torch.nn.Linear(32,16)\n","        # self.fully_connected_fifth = torch.nn.Linear(16,8)\n","        self.scoring_layer = torch.nn.Linear(16,1)\n","\n","  \n","\n","    def forward(self, data):\n","        \n","        utter_feature = torch.from_numpy(np.array(data[0]))\n","        context_feature = []\n","        speaker_feature = []\n","        show_feature = []\n","        context_speaker_feature = []\n","        scores = []\n","\n","\n","        utter_atten = self.attention(utter_feature)\n","\n","        if(args=='ut'):\n","          scores = utter_atten\n","          scores = torch.t(scores)\n","\n","        if(args=='ut-sp'):\n","          speaker_feature = torch.from_numpy(np.array(data[1])).view(-1,1)\n","          scores = torch.cat((utter_atten,speaker_feature),dim=0).view(1,-1)\n","        \n","        if(args=='ut-ct'):\n","          context_feature = torch.from_numpy(np.array(data[1]))\n","          context_atten = self.attention(context_feature)\n","          scores = utter_atten.add(context_atten)\n","          scores = torch.t(scores)\n","          \n","        if(args=='ut-ct-ext'):\n","          context_feature = torch.from_numpy(np.array(data[1]))\n","          context_atten = self.attention(context_feature)\n","          scores = torch.cat((utter_atten,context_atten[:50]),dim = 0).view(-1,1)\n","          scores = torch.t(scores)\n","          # print(scores.shape)\n","\n","        if(args=='ut-sh'):\n","          show_feature = torch.from_numpy(np.array(data[1])).view(-1,1)\n","          scores = torch.cat((utter_atten,show_feature),dim = 0).view(-1,1)\n","          scores = torch.t(scores)\n","\n","        if(args=='ut-sp-ct'):\n","          speaker_feature = torch.from_numpy(np.array(data[1])).view(-1,1)\n","          context_feature = torch.from_numpy(np.array(data[2]))\n","          context_atten = self.attention(context_feature)\n","          scores = utter_atten.add(context_atten)\n","          # scores = torch.cat((utter_atten,context_atten),dim = 0).view(-1,1)\n","          scores = torch.cat((scores,speaker_feature),dim=0).view(1,-1)\n","\n","        if(args=='ut-sp-ct-cp-sh'):\n","          speaker_feature = torch.from_numpy(np.array(data[1])).view(-1,1)\n","          context_feature = torch.from_numpy(np.array(data[2]))\n","          context_atten = self.attention(context_feature)\n","          context_speaker_feature = torch.from_numpy(np.array(data[3])).view(-1,1)\n","          show_feature = torch.from_numpy(np.array(data[4])).view(-1,1)\n","          scores = utter_atten.add(context_atten)\n","          # scores = torch.cat((utter_atten,context_atten),dim = 0).view(-1,1)\n","          scores = torch.cat((scores,speaker_feature),dim=0)\n","          # print(scores.shape)\n","          scores = torch.cat((scores,context_speaker_feature),dim=0)\n","          scores = torch.cat((scores,show_feature),dim=0).view(1,-1)\n","          # print(scores.shape)\n","\n","        score1 = torch.nn.functional.relu(self.fully_connected_first(scores))\n","        score2 = torch.nn.functional.relu(self.fully_connected_second(score1))\n","        score3 = torch.nn.functional.relu(self.fully_connected_third(score2))\n","        score4 = torch.nn.functional.relu(self.fully_connected_fourth(score3))\n","        # score5 = torch.nn.functional.relu(self.fully_connected_fifth(score4))\n","        score = torch.sigmoid(self.scoring_layer(score4))\n","        \n","        return score,score4\n"],"execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJsLLMywxLR3","executionInfo":{"status":"ok","timestamp":1606742055052,"user_tz":-330,"elapsed":3453,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":["class Trainer(object):\n","    \n","    def __init__(self, args,train_data,test_data,glove_dict):\n","        \n","        self.args = args\n","        self.train_data = train_data\n","        self.test_data = test_data\n","        self.glove_dict = glove_dict\n","        self.embedding_gen()\n","        self.setup_model()\n"," \n","    def setup_model(self):\n","        \n","        self.model = Model(self.args)\n"," \n","    def embedding_gen(self):\n","\n","      speaker_cnt = 0\n","      context_speaker_cnt  = 0\n","      show_cnt = 0\n","      \n","      speaker_dict = {}\n","      context_speaker_dict = {}\n","      show_dict = {}\n","      \n","      for sample in self.train_data[:,[1,3,4]]:\n","        speaker = sample[0]\n","        context_speakers = sample[1]\n","        show = sample[2]\n","        if(speaker not in speaker_dict):\n","          speaker_dict[speaker] = speaker_cnt\n","          speaker_cnt+=1\n","        \n","        for context_speaker in context_speakers:\n","          if(context_speaker not in context_speaker_dict):\n","            context_speaker_dict[context_speaker] = context_speaker_cnt\n","            context_speaker_cnt+=1\n","        \n","        if(show not in show_dict):\n","          show_dict[show] = show_cnt\n","          show_cnt+=1\n","\n","      for sample in self.test_data[:,[1,3,4]]:\n","        speaker = sample[0]\n","        context_speakers = sample[1]\n","        show = sample[2]\n","        if(speaker not in speaker_dict):\n","          speaker_dict[speaker] = speaker_cnt\n","          speaker_cnt+=1\n","        \n","        for context_speaker in context_speakers:\n","          if(context_speaker not in context_speaker_dict):\n","            context_speaker_dict[context_speaker] = context_speaker_cnt\n","            context_speaker_cnt+=1\n","        \n","        if(show not in show_dict):\n","          show_dict[show] = show_cnt\n","          show_cnt+=1\n","      print(speaker_dict)\n","      print(context_speaker_dict)\n","      print(show_dict)\n","      dim_speaker = len(speaker_dict)\n","      dim_context_speaker = len(context_speaker_dict)\n","      dim_show = len(show_dict)\n","\n","      print('speaker_cnt: ',dim_speaker)\n","      print('context_speaker_cnt: ',dim_context_speaker)\n","      print('show_cnt: ',dim_show)\n","      \n","       \n","      print(\"\\nGenerating for train\\n\")\n","\n","      label_train_data = []\n","      for sample in self.train_data[:,5]:\n","        label = sample\n","        label_train_data.append(label)\n","      \n","      label_train_data = np.array(label_train_data)\n","      # label_train_data.resize(len(self.train_data),1)\n","\n","      utter_train_data = []\n","      for sample in self.train_data[:,0]:\n","        feature_vec = []\n","        ut_text = sample\n","        sent_token = sent_tokenize(ut_text)\n","        for sent in sent_token:\n","          tokens = word_tokenize(sent)\n","          for token in tokens:\n","            feature_vec.append(self.glove_dict[token])\n","        \n","        utter_train_data.append(np.array(feature_vec))\n","\n","      utter_train_data = np.array(utter_train_data)\n","\n","      context_train_data = []\n","      for sample in self.train_data[:,2]:\n","        feature_vec = []\n","        for ct_sent in sample:\n","          ct_text = ct_sent\n","          sent_token = sent_tokenize(ct_text)\n","          for sent in sent_token:\n","            tokens = word_tokenize(sent)\n","            for token in tokens:\n","              feature_vec.append(self.glove_dict[token])\n","        context_train_data.append(np.array(feature_vec))\n","      \n","      context_train_data = np.array(context_train_data)\n","\n","      speaker_train_data = []\n","      for sample in self.train_data[:,1]:\n","        feature_vec = [0]*dim_speaker\n","        # print(feature_vec)\n","        speaker = sample\n","        feature_vec[speaker_dict[speaker]] = 1\n","        # print(feature_vec)\n","        # return\n","        speaker_train_data.append(np.array(feature_vec))\n","\n","      speaker_train_data = np.array(speaker_train_data)\n","\n","      context_speaker_train_data = []\n","      for sample in self.train_data[:,3]:\n","        feature_vec = [0]*dim_context_speaker\n","        context_speaker = sample\n","        for spk in context_speaker:\n","          feature_vec[context_speaker_dict[spk]] = 1\n","        \n","        context_speaker_train_data.append(np.array(feature_vec))\n","      \n","      context_speaker_train_data = np.array(context_speaker_train_data)\n","\n","      show_train_data = []\n","      for sample in self.train_data[:,4]:\n","        feature_vec = [0]*dim_show\n","        show = sample\n","        feature_vec[show_dict[show]] = 1\n","        show_train_data.append(np.array(feature_vec))\n","\n","      show_train_data = np.array(show_train_data)\n","      \n","\n","      print(\"\\nGenerating for test\\n\")\n","\n","      label_test_data = []\n","      for sample in self.test_data[:,5]:\n","        label = sample\n","        label_test_data.append(label)\n","\n","      label_test_data = np.array(label_test_data)\n","\n","      utter_test_data = []\n","      for sample in self.test_data[:,0]:\n","        feature_vec = []\n","        ut_text = sample\n","        sent_token = sent_tokenize(ut_text)\n","        for sent in sent_token:\n","          tokens = word_tokenize(sent)\n","          for token in tokens:\n","            feature_vec.append(self.glove_dict[token])\n","        utter_test_data.append(np.array(feature_vec))\n","\n","      utter_test_data = np.array(utter_test_data)\n","\n","      context_test_data = []\n","      for sample in self.test_data[:,2]:\n","        feature_vec = []\n","        for ct_sent in sample:\n","          ct_text = ct_sent\n","          sent_token = sent_tokenize(ct_text)\n","          for sent in sent_token:\n","            tokens = word_tokenize(sent)\n","            for token in tokens:\n","              feature_vec.append(self.glove_dict[token])\n","        context_test_data.append(np.array(feature_vec))\n","      \n","      context_test_data = np.array(context_test_data)\n","\n","      speaker_test_data = []\n","      for sample in self.test_data[:,1]:\n","        feature_vec = [0]*dim_speaker\n","        speaker = sample\n","        feature_vec[speaker_dict[speaker]] = 1\n","        speaker_test_data.append(np.array(feature_vec))\n","\n","      \n","      speaker_test_data = np.array(speaker_test_data)\n","\n","      context_speaker_test_data = []\n","      for sample in self.test_data[:,3]:\n","        feature_vec = [0]*dim_context_speaker\n","        context_speaker = sample\n","        for spk in context_speaker:\n","          feature_vec[context_speaker_dict[spk]] = 1\n","        \n","        context_speaker_test_data.append(np.array(feature_vec))\n","      \n","      context_speaker_test_data = np.array(context_speaker_test_data)\n","\n","      show_test_data = []\n","      for sample in self.test_data[:,4]:\n","        feature_vec = [0]*dim_show\n","        show = sample\n","        feature_vec[show_dict[show]] = 1\n","        show_test_data.append(np.array(feature_vec))\n","\n","      show_test_data = np.array(show_test_data)\n","\n","      self.train_dataset = []\n","      self.test_dataset = []\n","      \n","      if(args == 'ut'):  \n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],label_test_data[i]])\n","        \n","      elif(args == 'ut-sp'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],speaker_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],speaker_test_data[i],label_test_data[i]])\n","      \n","      elif(args == 'ut-ct' or args=='ut-ct-ext'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],context_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],context_test_data[i],label_test_data[i]])\n","        \n","      elif(args == 'ut-sh'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],show_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],show_test_data[i],label_test_data[i]])\n","\n","      elif(args=='ut-sp-ct'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],speaker_train_data[i],context_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],speaker_test_data[i],context_test_data[i],label_test_data[i]])\n","\n","\n","      elif(args=='ut-sp-cp'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],speaker_train_data[i],context_speaker_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],speaker_test_data[i],context_speaker_test_data[i],label_test_data[i]])\n","\n","      elif(args=='ut-sp-sh'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],speaker_train_data[i],show_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],speaker_test_data[i],show_test_data[i],label_test_data[i]])\n","\n","      elif(args == 'ut-sp-ct-cp-sh'):\n","        for i in range(len(self.train_data)):\n","          self.train_dataset.append([utter_train_data[i],speaker_train_data[i],context_train_data[i],context_speaker_train_data[i],show_train_data[i],label_train_data[i]])\n","        for i in range(len(self.test_data)):\n","          self.test_dataset.append([utter_test_data[i],speaker_test_data[i],context_test_data[i],context_speaker_test_data[i],show_test_data[i],label_test_data[i]])\n","\n","      self.train_dataset = np.array(self.train_dataset)\n","      self.test_dataset = np.array(self.test_dataset)\n","      print(self.train_dataset.shape)\n","      print(self.test_dataset.shape)\n"," \n","            \n","    def create_batches(self):\n","       \n","        \n","        mod_batches = []\n","        np.random.shuffle(self.train_dataset)\n","        \n","        ind = 0\n","        while(ind<len(self.train_dataset)):  \n","          batches = []\n","          for num in range(1):\n","            batches.append(self.train_dataset[ind])\n","            ind += 1\n","           \n","          mod_batches.append(batches)\n","        return mod_batches\n"," \n","    def process_batch(self, batch):\n","        \n","        self.optimizer.zero_grad()\n","        losses = 0\n","        # print('process_batch')\n","        # it = 0\n","        for sample in batch:\n","            data = sample[:len(sample)-1]\n","            label = sample[-1]\n","            result = torch.from_numpy(np.array(label).reshape(1,1)).view(-1).float() \n","            prediction,_ = self.model(data)\n","            losses = losses + torch.nn.functional.mse_loss(result, prediction)\n","        losses.backward(retain_graph=True)\n","        self.optimizer.step()\n","        loss = losses.item()\n","        return loss\n"," \n","    def fit(self):\n","      \n"," \n","        print(\"\\nModel training.\\n\")\n","        self.optimizer = torch.optim.Adam(self.model.parameters(),\n","                                          lr=0.001,\n","                                          weight_decay=5*10**-4)\n"," \n","        self.model.train()\n","        \n","        epochs = 150\n","        min_loss = 10000\n","        min_loss_epoch = 0\n","        self.min_loss_model = self.model\n","        for epoch in range(epochs):\n"," \n","            batches = self.create_batches()\n","            \n","            self.loss_sum = 0\n","            main_index = 0\n","            for \n","                loss_score = self.process_batch(batch)\n","                \n","                main_index = main_index + len(batch)\n","                # print(main_index)\n","                self.loss_sum = self.loss_sum + loss_score * len(batch)\n","                # self.loss_sum = self.loss_sum + loss_score\n","                loss = self.loss_sum/main_index\n","                \n","                \n","            # print(loss)\n","            if loss < min_loss:\n","                    min_loss = loss\n","                    min_loss_epoch = epoch\n","                    self.min_loss_model = self.model\n","                    print('min_loss: ',min_loss)\n","                    print(min_loss_epoch)\n","                    # print(self.min_loss_model)\n","            \n"," \n","        svm_feature = []\n","        # svm_target = []\n","        \n","        for sample in self.train_dataset:\n","          data = sample[:len(sample)-1]\n","            \n","          _,feature = self.min_loss_model(data)\n","          svm_feature.append(feature.tolist()[0])\n","          # svm_target.append(result)\n"," \n","        svm_target = []\n","        for sample in self.train_dataset:\n","          # data = graph_pair[:2]\n","          result = sample[-1]\n","          # _,feature = self.min_loss_model(data)\n","          svm_target.append(result)\n"," \n"," \n","        cnt0 = 0\n","        cnt1 = 0\n","        for i in svm_target:\n","          if(i==0):\n","            cnt0+=1\n","          else:\n","            cnt1+=1\n","        print(cnt0)\n","        print(cnt1)\n"," \n","        \n","        X = np.array(svm_feature)\n","        y = np.array(svm_target)\n","        print(y.shape)\n","        print(np.unique(y))\n","        from sklearn import svm\n","        from sklearn.metrics import confusion_matrix,accuracy_score\n","        self.svm_rbf_model = svm.SVC(kernel = 'rbf',gamma = 'scale', decision_function_shape = 'ovr')\n","        self.svm_rbf_model.fit(X,y)\n","        print(\"svm_model_trained\")\n","        #now adding SVM rbf classifier for removing step function\n","        \n","    \n"," \n","    \n","    def score(self):\n","        \n","        print(\"\\n\\nModel evaluation.\\n\")\n","        \n"," \n","        model1 = self.min_loss_model\n","        model1.eval()\n","        self.scores = []\n"," \n","        predictions = []\n","        true_label = []\n","        for sample in self.test_dataset:\n","            \n","            data = sample[:len(sample)-1]\n","            true_label.append(sample[-1])\n","            # result = torch.from_numpy(np.array(label).reshape(1,2)).view(-1).float() \n","            \n","            # print (type(graph_pair[2]))\n"," \n","            _,prediction = model1(data)\n","            predict = self.svm_rbf_model.predict(np.array(prediction.tolist())) \n","            predictions.append(predict)\n","        \n","        return true_label,predictions,model1,self.svm_rbf_model\n"," \n","       "],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6-g_CRJQxLWd","executionInfo":{"status":"error","timestamp":1606742073714,"user_tz":-330,"elapsed":21473,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}},"outputId":"e9ef0e29-80c0-42f6-f273-a10c4b1b16c6"},"source":["# print(type(data))\n","args = 'ut-sp-ct-cp-sh'\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm, trange\n","import pickle\n","from sklearn.metrics import precision_recall_fscore_support, classification_report\n","skf = StratifiedKFold(n_splits=5, shuffle=True)\n","# kf = KFold(n_splits=5)\n","# kf.get_n_splits(data)\n","results = []\n","accur = []\n","print(type(df))\n","max_f1 = 0\n","for train_index, test_index in skf.split(data,df['sarcasm'].to_numpy()):\n","# for train_index, test_index in kf.split(data):  \n","  train_data, test_data = data[train_index], data[test_index]\n","  print(train_data.shape)\n","  print(test_data.shape)\n","  \n","  trainer = Trainer(args,train_data,test_data,glove_dict)\n","  trainer.fit()\n","  y_test,y_pred,atten_model,svm_model = (trainer.score())\n","  # filename_atten = args+'_atten_model_50.sav'\n","  # filename_svm = args+'_svm_model_50.sav'\n","  # filename_atten = args+'_atten_model_75.sav'\n","  # filename_svm = args+'_svm_model_75.sav'\n","  # filename_atten = args+'_atten_model_100.sav'\n","  # filename_svm = args+'_svm_model_100.sav'\n","\n","  # filename_atten = args+'_atten_model_150.sav'\n","  # filename_svm = args+'_svm_model_150.sav'\n","  accur.append(accuracy_score(y_test, y_pred))\n","  results.append(precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n","\n","  # if(max_f1 < results[-1][2]):\n","  #   max_f1 = results[-1][2]\n","  #   pickle.dump(atten_model, open(filename_atten, 'wb'))\n","  #   pickle.dump(svm_model,open(filename_svm,'wb'))\n","  \n","  print(accur[-1])\n","  print(classification_report(y_test, y_pred))\n","  \n","\n","avg = [0,0,0,0]\n","for i in range(5):\n","    x, y, z, _ = results[i]\n","    avg[0] += x\n","    avg[1] += y\n","    avg[2] += z\n","    avg[3] += accur[i]\n","avg[0]/=5\n","avg[1]/=5\n","avg[2]/=5\n","avg[3]/=5\n","\n","\n","print(f\"Avg Accuracy: {avg[3]:.3f}\")\n","print(f\"Avg weighted precision: {avg[0]:.3f} :: Avg weighted recall: {avg[1]:.3f} :: Avg weighted F1: {avg[2]:.3f}\")\n","\n","\n","\n"],"execution_count":119,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","(552, 6)\n","(138, 6)\n","{'SHELDON': 0, 'HOWARD': 1, 'LEONARD': 2, 'RAJ': 3, 'PERSON': 4, 'PENNY': 5, 'AMY': 6, 'PERSON3': 7, 'BERNADETTE': 8, 'PERSON1': 9, 'CHANDLER': 10, 'ROSS': 11, 'MONICA': 12, 'JOEY': 13, 'RACHEL': 14, 'PHOEBE': 15, 'DOROTHY': 16, 'ROSE': 17, 'MEMBER-GIRL': 18, 'MODERATOR': 19, 'MEMBER-BOY': 20}\n","{'LEONARD': 0, 'SHELDON': 1, 'HOWARD': 2, 'PENNY': 3, 'PERSON': 4, 'RAJ': 5, 'PERSON1': 6, 'PERSON2': 7, 'BERNADETTE': 8, 'AMY': 9, 'JOEY': 10, 'CHANDLER': 11, 'ROSS': 12, 'MONICA': 13, 'RACHEL': 14, 'PHOEBE': 15, 'DOROTHY': 16, 'ROSE': 17, 'BLANCHE': 18, 'SOPHIA': 19, 'MODERATOR': 20, 'SCOTT': 21, 'MEMBER-BOY': 22, 'MEMBER-GIRL': 23}\n","{'BBT': 0, 'FRIENDS': 1, 'GOLDENGIRLS': 2, 'SARCASMOHOLICS': 3}\n","speaker_cnt:  21\n","context_speaker_cnt:  24\n","show_cnt:  4\n","\n","Generating for train\n","\n","\n","Generating for test\n","\n","(552, 6)\n","(138, 6)\n","\n","Model training.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:302: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"],"name":"stderr"},{"output_type":"stream","text":["min_loss:  0.2517807266904392\n","0\n","min_loss:  0.25049828138013464\n","1\n","min_loss:  0.2459788139609188\n","2\n","min_loss:  0.23910840533757885\n","3\n","min_loss:  0.2257591367440224\n","4\n","min_loss:  0.19994825129491764\n","5\n","min_loss:  0.16552663568770623\n","6\n","min_loss:  0.13452120971126472\n","7\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-119-bd91ebab73b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglove_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matten_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# filename_atten = args+'_atten_model_50.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-118-9f5eed94a242>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;31m# print(index,batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# print('batch_length: ',len(batch))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mloss_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# print('in iteration')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;31m# print('loss_Score:',loss_score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-118-9f5eed94a242>\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"D_8hjrTPxLZ0","executionInfo":{"status":"ok","timestamp":1606722082306,"user_tz":-330,"elapsed":445548,"user":{"displayName":"Abhishek Rajgaria","photoUrl":"","userId":"08629145772386760247"}}},"source":[""],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcH7zzYrxLUb"},"source":[""],"execution_count":null,"outputs":[]}]}